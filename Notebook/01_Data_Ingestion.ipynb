{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e0c08ac-20a7-470b-92f6-63883c3090bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root:     /Users/rellu/Documents/PTAB_Project/AI-Builders-Bootcamp-5/PTAB_Project3\n",
      "Notebook path:    /Users/rellu/Documents/PTAB_Project/AI-Builders-Bootcamp-5/PTAB_Project3/Notebook\n",
      "Data (raw):       /Users/rellu/Documents/PTAB_Project/AI-Builders-Bootcamp-5/PTAB_Project3/data/raw\n",
      "Data (processed): /Users/rellu/Documents/PTAB_Project/AI-Builders-Bootcamp-5/PTAB_Project3/data/processed\n",
      "Index:            /Users/rellu/Documents/PTAB_Project/AI-Builders-Bootcamp-5/PTAB_Project3/index\n",
      "Models:           /Users/rellu/Documents/PTAB_Project/AI-Builders-Bootcamp-5/PTAB_Project3/models\n"
     ]
    }
   ],
   "source": [
    "# Imports and project path setup\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "# Identify project root. \n",
    "cwd = Path.cwd()\n",
    "PROJECT_ROOT = cwd.parent if cwd.name == \"Notebook\" else cwd\n",
    "\n",
    "# Project root folder should be named \"PTAB_Project3\".\n",
    "if PROJECT_ROOT.name != \"PTAB_Project3\":\n",
    "    print(f\"⚠️ Note: expected project root to be 'PTAB_Project3' but found '{PROJECT_ROOT.name}'. Proceeding anyway.\")\n",
    "\n",
    "# Define key data paths that weill be used throughout the project.\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "RAW_DIR = DATA_DIR / \"raw\"\n",
    "PROCESSED_DIR = DATA_DIR / \"processed\"\n",
    "INDEX_DIR = PROJECT_ROOT / \"index\"\n",
    "MODELS_DIR = PROJECT_ROOT / \"models\"\n",
    "\n",
    "# Print paths to make sure the directories exist.\n",
    "for p in [DATA_DIR, RAW_DIR, PROCESSED_DIR, INDEX_DIR, MODELS_DIR]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Project root:    \", PROJECT_ROOT)\n",
    "print(\"Notebook path:   \", cwd)\n",
    "print(\"Data (raw):      \", RAW_DIR)\n",
    "print(\"Data (processed):\", PROCESSED_DIR)\n",
    "print(\"Index:           \", INDEX_DIR)\n",
    "print(\"Models:          \", MODELS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9fada58c-b5a6-4037-8c83-cd7454d6ab85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Querying PTAB for decisions from 2025-08-25 to 2025-09-01\n"
     ]
    }
   ],
   "source": [
    "# PTAB API setup + last-week date range\n",
    "\n",
    "from datetime import datetime, timedelta, timezone\n",
    "from pathlib import Path\n",
    "import requests\n",
    "\n",
    "# PTAB API base (v3 paths live under this host)\n",
    "PTAB_BASE = \"https://developer.uspto.gov/ptab-api\"\n",
    "\n",
    "# Save the PDFs the API returns in the raw_pdfs folder\n",
    "RAW_PDFS_DIR = DATA_DIR / \"raw_pdfs\"\n",
    "RAW_PDFS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "from datetime import datetime, timedelta, timezone\n",
    "\n",
    "def last_week_dates_yyyy_mm_dd(today=None):\n",
    "    \"\"\"\n",
    "    Returns (from_date, to_date) as strings 'YYYY-MM-DD' covering the last 7 days.\n",
    "    \"\"\"\n",
    "    # Use timezone-aware UTC date\n",
    "    if today is None:\n",
    "        today = datetime.now(timezone.utc).date()\n",
    "    to_date = today\n",
    "    from_date = today - timedelta(days=7)\n",
    "    return from_date.isoformat(), to_date.isoformat()\n",
    "\n",
    "FROM_DATE, TO_DATE = last_week_dates_yyyy_mm_dd()\n",
    "print(f\"Querying PTAB for decisions from {FROM_DATE} to {TO_DATE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "358388ce-ecb2-4c5b-b4cb-144a22c1d4e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched page 0: 25 rows (total so far: 25)\n",
      "\n",
      "✅ Total decisions fetched: 25\n",
      "\n",
      "Sample rows (up to 3):\n",
      "{\n",
      "  \"proceedingNumber\": \"2024001413\",\n",
      "  \"decisionDate\": \"08-25-2025\",\n",
      "  \"documentIdentifier\": \"3c30a4ca-c489-4853-87d7-2b698c11348b\"\n",
      "}\n",
      "{\n",
      "  \"proceedingNumber\": \"2024003328\",\n",
      "  \"decisionDate\": \"08-25-2025\",\n",
      "  \"documentIdentifier\": \"6e1d62ef-5a33-4f3f-a136-f490d83f0f25\"\n",
      "}\n",
      "{\n",
      "  \"proceedingNumber\": \"2024003699\",\n",
      "  \"decisionDate\": \"08-25-2025\",\n",
      "  \"documentIdentifier\": \"09abf05b-3b0e-484d-8817-0fd2ee3dc740\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# USPTO PTAB API Call: fetch PTAB decisions within [FROM_DATE, TO_DATE], with pagination\n",
    "\n",
    "import time\n",
    "import json\n",
    "from typing import List, Dict, Any\n",
    "import requests\n",
    "\n",
    "def fetch_decisions(from_date: str, to_date: str, page_size: int = 100, max_pages: int = 50) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Call the USPTO PTAB API /decisions endpoint to list decisions in a date range.\n",
    "    - Handles pagination with 'page' and 'size' style params.\n",
    "    - Returns a flat list of 'decision' records (dicts as provided by the API).\n",
    "    \"\"\"\n",
    "    all_rows: List[Dict[str, Any]] = []\n",
    "    page = 0\n",
    "\n",
    "    while page < max_pages:\n",
    "        params = {\n",
    "            \"decisionFromDate\": from_date,  # inclusive\n",
    "            \"decisionToDate\": to_date,      # inclusive\n",
    "            \"size\": page_size,\n",
    "            \"page\": page,\n",
    "        }\n",
    "        url = f\"{PTAB_BASE}/decisions\"\n",
    "        try:\n",
    "            resp = requests.get(url, params=params, timeout=30)\n",
    "            resp.raise_for_status()\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"❌ Request error on page {page}: {e}\")\n",
    "            break\n",
    "\n",
    "        data = resp.json()\n",
    "\n",
    "        # defensive parsing: try common fields, fall back gracefully\n",
    "        rows = []\n",
    "        for key in (\"decisions\", \"results\", \"items\", \"content\"):\n",
    "            if isinstance(data.get(key), list):\n",
    "                rows = data[key]\n",
    "                break\n",
    "\n",
    "        # if the API returns the list directly (rare), accept that too\n",
    "        if not rows and isinstance(data, list):\n",
    "            rows = data\n",
    "\n",
    "        if not rows:\n",
    "            # either no results or unexpected shape; show keys for debugging\n",
    "            print(f\"⚠️ No rows found on page {page}. Top-level keys: {list(data.keys())}\")\n",
    "            break\n",
    "\n",
    "        all_rows.extend(rows)\n",
    "        print(f\"Fetched page {page}: {len(rows)} rows (total so far: {len(all_rows)})\")\n",
    "\n",
    "        # stop if this page wasn't full (likely last page)\n",
    "        if len(rows) < page_size:\n",
    "            break\n",
    "\n",
    "        page += 1\n",
    "        time.sleep(0.2)  # tiny pause to be polite\n",
    "\n",
    "    return all_rows\n",
    "\n",
    "# ---- run the fetch for your last-week window ----\n",
    "decisions_last_week = fetch_decisions(FROM_DATE, TO_DATE, page_size=100)\n",
    "\n",
    "print(f\"\\n✅ Total decisions fetched: {len(decisions_last_week)}\")\n",
    "\n",
    "# Show a small preview of useful fields if present\n",
    "preview_keys = [\"proceedingNumber\", \"decisionDate\", \"documentIdentifier\", \"documentId\", \"partyName\", \"applicationNumber\", \"technologyCenter\"]\n",
    "print(\"\\nSample rows (up to 3):\")\n",
    "for row in decisions_last_week[:3]:\n",
    "    sample = {k: row.get(k) for k in preview_keys if k in row}\n",
    "    print(json.dumps(sample, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0574844c-cc5c-484c-bf2e-fa279c6edfd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📄 PDFs saved: 0, skipped (already present): 25, failed: 0\n",
      "Files are in: /Users/rellu/Documents/PTAB_Project/AI-Builders-Bootcamp-5/PTAB_Project3/data/raw_pdfs\n"
     ]
    }
   ],
   "source": [
    "# Download decision PDFs to data/raw_pdfs folder\n",
    "\n",
    "import os\n",
    "import re\n",
    "from typing import Optional\n",
    "\n",
    "def sanitize(s: str) -> str:\n",
    "    \"\"\"Make a string safe for filenames: keep alphanumerics, dash, underscore.\"\"\"\n",
    "    return re.sub(r\"[^A-Za-z0-9_\\-]+\", \"_\", s).strip(\"_\")\n",
    "\n",
    "def make_pdf_filename(proceeding: str, decision_date: str, doc_id: str) -> str:\n",
    "    \"\"\"\n",
    "    Create a readable filename like:\n",
    "      2024001413_2025-08-25_3c30a4ca-c489-4853-87d7-2b698c11348b.pdf\n",
    "    \"\"\"\n",
    "    # Try to normalize date to YYYY-MM-DD if it’s like MM-DD-YYYY\n",
    "    d = decision_date\n",
    "    if re.match(r\"^\\d{2}-\\d{2}-\\d{4}$\", d):  # MM-DD-YYYY\n",
    "        mm, dd, yyyy = d.split(\"-\")\n",
    "        d = f\"{yyyy}-{mm}-{dd}\"\n",
    "    return f\"{sanitize(proceeding)}_{sanitize(d)}_{sanitize(doc_id)}.pdf\"\n",
    "\n",
    "def download_pdf(document_identifier: str, out_path: Path, timeout: int = 60) -> bool:\n",
    "    \"\"\"\n",
    "    Download a single PDF by documentIdentifier into out_path.\n",
    "    Returns True if saved, False otherwise.\n",
    "    \"\"\"\n",
    "    url = f\"{PTAB_BASE}/documents/{document_identifier}/download\"\n",
    "    try:\n",
    "        with requests.get(url, stream=True, timeout=timeout) as r:\n",
    "            r.raise_for_status()\n",
    "            # simple content-type check (sometimes it's octet-stream)\n",
    "            ctype = (r.headers.get(\"Content-Type\") or \"\").lower()\n",
    "            if \"pdf\" not in ctype and \"octet-stream\" not in ctype:\n",
    "                print(f\"⚠️ Unexpected content-type for {document_identifier}: {ctype}\")\n",
    "            # write to disk in chunks\n",
    "            with open(out_path, \"wb\") as f:\n",
    "                for chunk in r.iter_content(chunk_size=8192):\n",
    "                    if chunk:\n",
    "                        f.write(chunk)\n",
    "        return True\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"❌ Download failed for {document_identifier}: {e}\")\n",
    "        return False\n",
    "\n",
    "saved, skipped, failed = 0, 0, 0\n",
    "\n",
    "for row in decisions_last_week:\n",
    "    doc_id = row.get(\"documentIdentifier\")\n",
    "    proceeding = row.get(\"proceedingNumber\") or \"unknown\"\n",
    "    decision_date = row.get(\"decisionDate\") or \"unknown-date\"\n",
    "    if not doc_id:\n",
    "        # if this happens, we could later add a \"list documents by proceeding\" step\n",
    "        print(f\"⚠️ No documentIdentifier for proceeding {proceeding}; skipping.\")\n",
    "        continue\n",
    "\n",
    "    fname = make_pdf_filename(proceeding, decision_date, doc_id)\n",
    "    out_path = RAW_PDFS_DIR / fname\n",
    "\n",
    "    if out_path.exists() and out_path.stat().st_size > 0:\n",
    "        skipped += 1\n",
    "        continue\n",
    "\n",
    "    ok = download_pdf(doc_id, out_path)\n",
    "    if ok and out_path.exists() and out_path.stat().st_size > 1024:\n",
    "        saved += 1\n",
    "    else:\n",
    "        # remove zero-byte or tiny files if any\n",
    "        try:\n",
    "            if out_path.exists():\n",
    "                out_path.unlink()\n",
    "        except Exception:\n",
    "            pass\n",
    "        failed += 1\n",
    "\n",
    "print(f\"\\n📄 PDFs saved: {saved}, skipped (already present): {skipped}, failed: {failed}\")\n",
    "print(f\"Files are in: {RAW_PDFS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78829fac-9b27-4069-89c4-19db3454b798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 26 PDFs in /Users/rellu/Documents/PTAB_Project/AI-Builders-Bootcamp-5/PTAB_Project3/data/raw_pdfs\n",
      "📝 Wrote raw text: 2024000857_2025-08-26_75d26863-bf26-41d9-92c5-17605cea3c51.txt\n",
      "📝 Wrote raw text: 2024001050_2025-08-26_96b1de0c-7637-4aeb-b290-ea76af8a4087.txt\n",
      "📝 Wrote raw text: 2024001413_2025-08-25_3c30a4ca-c489-4853-87d7-2b698c11348b.txt\n",
      "📝 Wrote raw text: 2024002039_2025-08-25_8cf23631-8e2c-4060-a7c4-534b97087a2d.txt\n",
      "📝 Wrote raw text: 2024002880_2025-08-26_6583dc03-0b34-46bd-a958-93345eb5e15c.txt\n",
      "📝 Wrote raw text: 2024003101_2025-08-26_0d0f2c69-9dc8-4643-a019-914b4792e551.txt\n",
      "📝 Wrote raw text: 2024003328_2025-08-25_6e1d62ef-5a33-4f3f-a136-f490d83f0f25.txt\n",
      "📝 Wrote raw text: 2024003687_2025-08-25_4d318724-04f0-46ec-acb8-3a93d92886bf.txt\n",
      "📝 Wrote raw text: 2024003699_2025-08-25_09abf05b-3b0e-484d-8817-0fd2ee3dc740.txt\n",
      "📝 Wrote raw text: 2024003717_2025-08-26_60b52718-384b-44de-abeb-c7e7a9816963.txt\n",
      "📝 Wrote raw text: 2024003965_2025-08-27_d8cc0133-42e0-4bb3-bd78-6badfcc759d0.txt\n",
      "📝 Wrote raw text: 2024003977_2025-08-27_94f3e88e-18f4-49b2-a88b-568eba5a9448.txt\n",
      "📝 Wrote raw text: 2024004066_2025-08-27_c3f3088b-7af5-4ddc-b021-d1216c8bf7bd.txt\n",
      "📝 Wrote raw text: 2024004072_2025-08-25_b30245b5-2908-4020-9060-4cb28aad4ac5.txt\n",
      "📝 Wrote raw text: 2024004096_2025-08-27_ca8b7daf-07f0-4741-98b4-f504184e3399.txt\n",
      "📝 Wrote raw text: 2024004106_2025-08-27_32f45f1a-66d2-4e87-82dc-67fa1196f3a5.txt\n",
      "📝 Wrote raw text: 2025000088_2025-08-27_a942d5b6-d9a3-4e1c-a837-14614eeea12b.txt\n",
      "📝 Wrote raw text: 2025000110_2025-08-25_da1b0163-4586-48fc-868e-fd4318312577.txt\n",
      "📝 Wrote raw text: 2025000420_2025-08-26_241188d6-7a7c-414e-8eeb-780db6d139e0.txt\n",
      "📝 Wrote raw text: 2025000477_2025-08-27_4945595a-c570-4d55-a6ce-c2b25564b49d.txt\n",
      "📝 Wrote raw text: 2025000819_2025-08-25_7294b159-c4dc-4aa0-a8d9-b9e6eb178bc4.txt\n",
      "📝 Wrote raw text: 2025000834_2025-08-26_726f0f94-d874-4409-ae6d-b9e141e2eb43.txt\n",
      "📝 Wrote raw text: 2025001586_2025-08-26_b798c2dd-0e23-45ab-a452-cf5aa9c4a720.txt\n",
      "📝 Wrote raw text: 2025002136_2025-08-27_6fd932ae-6201-47a1-adba-a5931a43b24d.txt\n",
      "📝 Wrote raw text: IPR2024-00577_2025-08-26_171168102.txt\n",
      "📝 Wrote raw text: IPR2024-00736_2025-08-25_171166608.txt\n",
      "\n",
      "✅ Done converting PDFs.\n"
     ]
    }
   ],
   "source": [
    "# Convert raw PDFs to raw .txt files\n",
    "\n",
    "from pypdf import PdfReader\n",
    "\n",
    "RAW_PDFS_DIR = DATA_DIR / \"raw_pdfs\"\n",
    "RAW_PDFS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def extract_pdf_text(pdf_path: Path, max_pages: int | None = None) -> str:\n",
    "    \"\"\"\n",
    "    Extract text from a PDF using pypdf.\n",
    "    - max_pages: limit pages to speed things up (or None = all pages).\n",
    "    \"\"\"\n",
    "    reader = PdfReader(str(pdf_path))\n",
    "    pages = reader.pages\n",
    "    n = len(pages) if max_pages is None else min(len(pages), max_pages)\n",
    "    chunks = []\n",
    "    for i in range(n):\n",
    "        t = pages[i].extract_text() or \"\"\n",
    "        if t.strip():\n",
    "            chunks.append(t)\n",
    "    return \"\\n\\n\".join(chunks)\n",
    "\n",
    "pdf_files = sorted(RAW_PDFS_DIR.glob(\"*.pdf\"))\n",
    "print(f\"Found {len(pdf_files)} PDFs in {RAW_PDFS_DIR}\")\n",
    "\n",
    "for pdf_path in pdf_files:\n",
    "    # Use the filename (without extension) as the document id and title\n",
    "    doc_id = pdf_path.stem\n",
    "    title = doc_id.replace(\"_\", \" \").replace(\"-\", \" \").strip()\n",
    "\n",
    "    # Extract text (you can raise max_pages or set to None if you want full docs)\n",
    "    raw_text = extract_pdf_text(pdf_path, max_pages=20)\n",
    "\n",
    "    if not raw_text.strip():\n",
    "        print(f\"⚠️ Skipping (no text extracted): {pdf_path.name}\")\n",
    "        continue\n",
    "\n",
    "    # Save to RAW_DIR as a .txt file (first line = title, then body)\n",
    "    out_txt = RAW_DIR / f\"{doc_id}.txt\"\n",
    "    out_txt.write_text(f\"{title}\\n\\n{raw_text}\", encoding=\"utf-8\")\n",
    "    print(f\"📝 Wrote raw text: {out_txt.name}\")\n",
    "\n",
    "print(\"\\n✅ Done converting PDFs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28bbcee1-eb96-4402-b15f-0443119ba9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions that will be used for ingesting pdfs\n",
    "\n",
    "from typing import List, Tuple\n",
    "import re\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Light cleanup for legal PDFs/text:\n",
    "    - normalize Mac line endings to '\\n'\n",
    "    - collapse 3+ newlines to 2 (keeps paragraphs but removes huge gaps)\n",
    "    - fix hyphenated line breaks like 'compu-\\ntation' -> 'computation'\n",
    "    - collapse multiple spaces to one\n",
    "    - strip leading/trailing whitespace\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "\n",
    "    # normalize newlines\n",
    "    t = text.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
    "\n",
    "    # fix common hyphenation across line breaks: word-\\nword -> wordword\n",
    "    t = re.sub(r\"(\\w)-\\n(\\w)\", r\"\\1\\2\", t)\n",
    "\n",
    "    # collapse 3+ newlines -> 2 newlines (paragraph spacing)\n",
    "    t = re.sub(r\"\\n{3,}\", \"\\n\\n\", t)\n",
    "\n",
    "    # collapse runs of spaces/tabs\n",
    "    t = re.sub(r\"[ \\t]{2,}\", \" \", t)\n",
    "\n",
    "    # strip extra space around newlines\n",
    "    t = re.sub(r\"[ \\t]*\\n[ \\t]*\", \"\\n\", t)\n",
    "\n",
    "    return t.strip()\n",
    "\n",
    "\n",
    "def list_raw_text_files(raw_dir=RAW_DIR) -> List[Path]:\n",
    "    \"\"\"\n",
    "    Return a list of .txt files currently in data/raw.\n",
    "    Use this to see what input there is in raw folder before processing.\n",
    "    \"\"\"\n",
    "    return sorted(raw_dir.glob(\"*.txt\"))\n",
    "\n",
    "\n",
    "def save_processed_text(doc_id: str, title: str, body: str, processed_dir=PROCESSED_DIR) -> Path:\n",
    "    \"\"\"\n",
    "    Save a cleaned document into data/processed as a simple UTF-8 .txt.\n",
    "    Conventions:\n",
    "      - First line = title\n",
    "      - Blank line\n",
    "      - Body text\n",
    "    Returns the saved Path.\n",
    "    \"\"\"\n",
    "    processed_dir.mkdir(parents=True, exist_ok=True)\n",
    "    out_path = processed_dir / f\"{doc_id}.txt\"\n",
    "    content = f\"{title.strip()}\\n\\n{body.strip()}\\n\"\n",
    "    out_path.write_text(content, encoding=\"utf-8\")\n",
    "    return out_path\n",
    "\n",
    "\n",
    "def read_txt(path: Path) -> Tuple[str, str]:\n",
    "    \"\"\"\n",
    "    Read a .txt file and split into (title, body).\n",
    "    If the first line is empty, we treat the filename stem as title.\n",
    "    \"\"\"\n",
    "    raw = path.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "    lines = raw.splitlines()\n",
    "    if lines:\n",
    "        title = lines[0].strip() or path.stem\n",
    "        body = \"\\n\".join(lines[1:]).strip()\n",
    "    else:\n",
    "        title, body = (path.stem, \"\")\n",
    "    return title, body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0c30c6c-dfae-4c76-9ba3-5a5210084fa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 27 raw text files.\n",
      "Processed and saved: 2024000857_2025-08-26_75d26863-bf26-41d9-92c5-17605cea3c51.txt\n",
      "Processed and saved: 2024001050_2025-08-26_96b1de0c-7637-4aeb-b290-ea76af8a4087.txt\n",
      "Processed and saved: 2024001413_2025-08-25_3c30a4ca-c489-4853-87d7-2b698c11348b.txt\n",
      "Processed and saved: 2024002039_2025-08-25_8cf23631-8e2c-4060-a7c4-534b97087a2d.txt\n",
      "Processed and saved: 2024002880_2025-08-26_6583dc03-0b34-46bd-a958-93345eb5e15c.txt\n",
      "Processed and saved: 2024003101_2025-08-26_0d0f2c69-9dc8-4643-a019-914b4792e551.txt\n",
      "Processed and saved: 2024003328_2025-08-25_6e1d62ef-5a33-4f3f-a136-f490d83f0f25.txt\n",
      "Processed and saved: 2024003687_2025-08-25_4d318724-04f0-46ec-acb8-3a93d92886bf.txt\n",
      "Processed and saved: 2024003699_2025-08-25_09abf05b-3b0e-484d-8817-0fd2ee3dc740.txt\n",
      "Processed and saved: 2024003717_2025-08-26_60b52718-384b-44de-abeb-c7e7a9816963.txt\n",
      "Processed and saved: 2024003965_2025-08-27_d8cc0133-42e0-4bb3-bd78-6badfcc759d0.txt\n",
      "Processed and saved: 2024003977_2025-08-27_94f3e88e-18f4-49b2-a88b-568eba5a9448.txt\n",
      "Processed and saved: 2024004066_2025-08-27_c3f3088b-7af5-4ddc-b021-d1216c8bf7bd.txt\n",
      "Processed and saved: 2024004072_2025-08-25_b30245b5-2908-4020-9060-4cb28aad4ac5.txt\n",
      "Processed and saved: 2024004096_2025-08-27_ca8b7daf-07f0-4741-98b4-f504184e3399.txt\n",
      "Processed and saved: 2024004106_2025-08-27_32f45f1a-66d2-4e87-82dc-67fa1196f3a5.txt\n",
      "Processed and saved: 2025000088_2025-08-27_a942d5b6-d9a3-4e1c-a837-14614eeea12b.txt\n",
      "Processed and saved: 2025000110_2025-08-25_da1b0163-4586-48fc-868e-fd4318312577.txt\n",
      "Processed and saved: 2025000420_2025-08-26_241188d6-7a7c-414e-8eeb-780db6d139e0.txt\n",
      "Processed and saved: 2025000477_2025-08-27_4945595a-c570-4d55-a6ce-c2b25564b49d.txt\n",
      "Processed and saved: 2025000819_2025-08-25_7294b159-c4dc-4aa0-a8d9-b9e6eb178bc4.txt\n",
      "Processed and saved: 2025000834_2025-08-26_726f0f94-d874-4409-ae6d-b9e141e2eb43.txt\n",
      "Processed and saved: 2025001586_2025-08-26_b798c2dd-0e23-45ab-a452-cf5aa9c4a720.txt\n",
      "Processed and saved: 2025002136_2025-08-27_6fd932ae-6201-47a1-adba-a5931a43b24d.txt\n",
      "Processed and saved: IPR2024-00577_2025-08-26_171168102.txt\n",
      "Processed and saved: IPR2024-00736_2025-08-25_171166608.txt\n",
      "Processed and saved: demo_case.txt\n",
      "\n",
      "✅ All raw files processed into data/processed/\n"
     ]
    }
   ],
   "source": [
    "# Process all raw files into Data/Processed folders\n",
    "\n",
    "raw_files = list_raw_text_files()\n",
    "print(f\"Found {len(raw_files)} raw text files.\")\n",
    "\n",
    "for path in raw_files:\n",
    "    # extract title + body from the raw file\n",
    "    title, body = read_txt(path)\n",
    "    \n",
    "    # clean up the body text\n",
    "    cleaned_body = clean_text(body)\n",
    "    \n",
    "    # use the stem (filename without extension) as document id\n",
    "    doc_id = path.stem\n",
    "    \n",
    "    # save into processed folder\n",
    "    out_path = save_processed_text(doc_id, title, cleaned_body)\n",
    "    print(f\"Processed and saved: {out_path.name}\")\n",
    "\n",
    "print(\"\\n✅ All raw files processed into data/processed/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d508c26b-d97f-4e05-b9a5-93a6c5f512ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PTAB Project 3",
   "language": "python",
   "name": "ptab3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
